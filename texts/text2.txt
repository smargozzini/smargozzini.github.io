Para las lecturas de esta semana se pudo apreciar que los dos papers estudiaban métodos de factorización de matrices. Uno combinaba métodos para predecir ítems que le gustarían a un usuario y comparar los resultados con los que se encuentran publicados por distintos usuarios. Uno de los resultados que se comparaban, eran los obtenidos por personas que concursaron en el netflix price. El otro paper usaba métodos para recomendar programas que podrían gustarles a los usuarios en base a lo que ya vieron.

De los dos papers me llamo la intención en que se centran mucho en varias cosas:

-	La primera es que se dan vueltas en encontrar los valores óptimos para el learning rate y para el factor de regularización. Luego de leer estos papers me di cuenta de que estos valores no pueden ser aleatorios y darles un peso significativo puede influir mucho en las recomendaciones obtenidas, lo cual influye directamente en la satisfacción del usuario al usar nuestro programa.
-	Otro tema al que la dan varias vueltas son los posibles sesgos que pueden existir en los datos con los que se entrenan. Por ejemplo, el paper de los programas de televisión proponen alternativas de pesos para programas que se vieren en el mismo canal de manera consecutiva, asumiendo que quizás el usuario se quedo dormido o dejo la televisión prendida y se fue. 
-	Lo ultimo que me llamo la atención que tienen en común estos dos papers fue la preocupación en la escalabilidad y el tiempo requerido para entrenar sus modelos. Ambos modelos eran capaces de procesar una cantidad grande de datos en un tiempo no mayor a una hora (no me acuerdo perfectamente, pero creo que se demoraban alrededor de 20 minutos) y esto era una de las ventajas mas grandes que tenían sus modelos, ya que se pueden actualizar sus vectores en un periodo de tiempo muy corto.
